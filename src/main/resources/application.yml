# 配置端口号
server:
  port: 8080
swagger:
  enabled: true
# 数据源配置
spring:
  application:
    name: integral-module
  # Kafka 配置项，对应 KafkaProperties 配置类
  kafka:
    # 指定 Kafka Broker 地址，可以设置多个，以逗号分隔
    bootstrap-servers: 192.168.68.131:9092
    # Kafka Producer 配置项
    producer:
      acks: 1 # 0-不应答。1-leader 应答。all-所有 leader 和 follower 应答。
      retries: 3 # 发送失败时，重试发送的次数
      key-serializer: org.apache.kafka.common.serialization.StringSerializer # 消息的 key 的序列化
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer # 消息的 value 的序列化
    # Kafka Consumer 配置项
    consumer:
      group-id: test #群组ID
      enable-auto-commit: true  # 取消自动提交
      auto-commit-interval: 1000
      auto-offset-reset: earliest # 设置消费者分组最初的消费进度为 earliest
      #密钥的反序列化器类，实现类实现了接口org.apache.kafka.common.serialization.Deserializer
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
     #值的反序列化器类，实现类实现了接口org.apache.kafka.common.serialization.Deserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      max-poll-records: 1000
    #      properties:
#        spring:
#          json:
#            trusted:
#              packages: com.artisan.springkafka.domain
    # Kafka Consumer Listener 监听器配置
    listener:
      missing-topics-fatal: false # 消费监听接口监听的主题不存在时，默认会报错。所以通过设置为 false ，解决报错
      type=batch: batch
  datasource:
    type: com.alibaba.druid.pool.DruidDataSource
    driverClassName: com.mysql.cj.jdbc.Driver
    druid:
      first:  #数据源1
        url: jdbc:mysql://localhost:3306/jeecg-boot?useUnicode=true&characterEncoding=utf-8&useSSL=true&serverTimezone=UTC&nullCatalogMeansCurrent=true
        username: wuhf
        password: whf123
      second:  #数据源2
        url: jdbc:mysql://10.10.168.18:3306/jeecg-boot??useUnicode=true&characterEncoding=utf-8&useSSL=true&serverTimezone=UTC&nullCatalogMeansCurrent=true
        username: wuhf
        password: whf123
      initial-size: 10
      max-active: 100
      min-idle: 10
      max-wait: 60000
      pool-prepared-statements: true
      max-pool-prepared-statement-per-connection-size: 20
      time-between-eviction-runs-millis: 60000
      min-evictable-idle-time-millis: 300000
      #validation-query: SELECT 1 FROM DUAL
      test-while-idle: true
      test-on-borrow: false
      test-on-return: false
      stat-view-servlet:
        enabled: true
        url-pattern: /druid/*
        #login-username: admin
        #login-password: admin
      filter:
        stat:
          log-slow-sql: true
          slow-sql-millis: 1000
          merge-sql: false
        wall:
          config:
            #允许一次执行多条语句
            multi-statement-allow: true

# mybatis- plus配置
mybatis-plus:
  # xml扫描，多个目录用逗号或者分号隔开隔开
  mapper-locations: classpath:mapper/*.xml
  #实体扫描，多个package用逗号或者分号分隔
  typeAliasesPackage: com.galaxy.portal.*.entity
  # 以下配置均有默认值,可以不设置
  global-config:
    db-config:
      #主键类型 AUTO:"数据库ID自增" INPUT:"用户输入ID",ID_WORKER:"全局唯一ID (数字类型唯一ID)", UUID:"全局唯一ID UUID";
      id-type: auto
      #字段策略 0:"忽略判断",1:"非 NULL 判断"),2:"非空判断"
      field-strategy: 2
      #驼峰下划线转换
      db-column-underline: true
      #刷新mapper 调试神器
      refresh-mapper: true
  configuration:
    # 是否开启自动驼峰命名规则映射:从数据库列名到Java属性驼峰命名的类似映射
    map-underscore-to-camel-case: true
    # 返回map时true:当查询数据为空时字段返回为null,false:不加这个查询数据为空时，字段将被隐藏
    call-setters-on-nulls: true
    # 这个配置会将执行的sql打印出来，在开发或测试的时候可以用
    log-impl: org.apache.ibatis.logging.stdout.StdOutImpl


##注册到eruka上面
eureka:
  client:
    register-with-eureka: true
    fetchRegistry: true
    service-url:
      defaultZone: http://eureka7001.com:7001/eureka/,http://eureka7002.com:7002/eureka/

feign:
  hystrix:
    enabled: true
hystrix:
  command:
    default: #也可以针对多个服务
      execution:
        isolation:
          thread:
            timeoutInMilliseconds: 4000 # 设置hystrix的超时时间为4000ms

##设置feign客户端超时时间（OpenFiegn默认支持ribbon）
ribbon:
  # 建立连接后读取可用资源所用的时间
  ReadTimeout: 5000
  # 建立连接所用时间
  ConnectTimeout: 5000


analysis:
  file:
    splitStr: \|
    topic: test01
    threadNumber: 10
    titleRowNum: 2
    dataStartRowNum: 4
    analysisedMovePath: D:\data\ceshi2
  scan:
    switch: false
    dirPath: D:\data\ceshi
    #1、文件格式：数据要插入的表名-new/append(新增/追加)-日期（yyyyMMdd）-计数.txt
    fileNameFormat: ^[A-Za-z0-9]+-[\u4E00-\u9FA5A-Za-z0-9]+-[new|append]+-(\d{2}|\d{4})(0?[1-9]|1[12])(0?[1-9]|[12]\d|3[01])-\d{1,}[\.]txt$

es:
  host: 192.168.32.1
  port: 9200
  scheme: http